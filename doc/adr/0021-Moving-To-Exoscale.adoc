= Moving to Exocale

Date: 2024-08-28

== Status

Exploring & Learning

== Context

=== Background
Cljdoc has been using a DigitalOcean droplet, AWS Route 53, AWS S3 for some time.

This comes with the following characteristics:

1. Only Martin can carry out host-level ops, these are all on personal accounts
2. There is reasonable costs associated with these services
3. While we are good with keeping up with docker level updates, our host-level ops is in neglect

A while back, Exoscale generously offered to cover hosting of cljdoc.
I recently asked them if they were still willing and, good news for us, they answered in the affirmative.

Moving to Exoscale will:

1. Allow Lee to become more familiar with and help out with host-level ops
2. Move hosting costs to Exoscale, which means one less thing for us to worry about.
3. As a team of 2, we have more of a fighting chance to keep our host-level ops in order

== Exoscale Services
Exoscale has a rich set of services.
Our current architecture maps like so:

* DigitalOcean Droplet -> Exoscale Compute Standard Medium (with 50gb storage)
* AWS Route 53 -> Exoscale DNS
* AWS S3 -> Exoscale Object Store

Exoscale also has other services, like SKS (Kubernetes) DBAAS (PostgreSQL, MySQL, etc), Block Storage, Elastic IPs, and so on.

== Cljdoc Architecture at Exoscale
Cljdoc's current architecture is optimized to minimize financial cost.
It runs on a single DigitalOcean Droplet and uses a local SQLite database.
This has some cons:

1. Updating the host server is difficult.
We have automatic blue/green deployment but at the docker level.
If blue/green were at the host level, we'd have a much easier time keeping our host up to date.
2. The SQLite database is awesome in that it is local and fast.
But it being local, means upgrading the host requires some careful thought.
3. We use tools like traefik, nomad and consul to manage blue/green deployments.
Keeping these tools up to date has a maintenance cost.

It is tempting to chuck our current architecture when moving to Exoscale, but that requires more thought and more time, and delays us moving to Exoscale.
So we'll, within reason, keep our current archecture to start with.

== Progress

- [done] Fire up a Compute instance (debian 12) manually at Exoscale, ssh in and poke around.
- [done] Setup 2fa at Exoscale
- [done] Install, and try out, `exo` command line tool
- [done] Explore Packer vs Cloud Init (currently like Packer for big setup)
- Explore terraform
  - [done] Bring up Compute instance
  - [done] Try defining ssh keys
  - Bring up Object Store
  - Bring up DNS (how can I experiment here? cljdoc.xyz? buy a cheap domain?)
  - Figure out how to setup a static IP

== Finer Questions

=== Terraform
We'll continue to use terraform to declare and provision cloud services.
Exoscale has support for terraform: https://registry.terraform.io/providers/exoscale/exoscale/latest/docs

Because we want to be an ops team I'd like to somehow share terraform state.
Terraform state is sensitive, so we'd need to share it securely.
And we'd like to avoid the possibility of concurrent updates.

Terraform supports saving state to s3 via `backend` config.
https://developer.hashicorp.com/terraform/language/settings/backends/s3
Clojars makes use of this feature:
https://github.com/clojars/infrastructure/blob/6cf9c100e38408016cd979f1611602523766200e/terraform/main.tf#L6-L11

Exoscale includes an example of doing this.
https://github.com/exoscale/terraform-provider-exoscale/blob/aef50d3f097648d405bcca1a46c8a99959f94706/examples/sos-backend/providers.tf

When using s3, locking is currently optionally supported via dynamoDB,
We don't have dynamoDB at Exoscale, so that's a nogo.
But there is some recent investigation into supporting locking via new s3 conditional writes.
See: https://github.com/hashicorp/terraform/issues/35625

Terraform s3 backend also optionally supports encryption for data at rest.
https://developer.hashicorp.com/terraform/language/settings/backends/s3#encrypt
But.. I think this might be via s3 encryption.
https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html
Which might not be currenty available for Exoscale Oject Store.
https://community.exoscale.com/documentation/storage/encryption/#encryption-at-rest

=== Access
Exoscale supports ssh access to the host.
Although we don't want to make changes to the host directly, it can be convenient to poke around.

TODO: Ensure both Martin and I have access.
TODO: Once I get something basic going invite Martin to the cljdoc org at  Exoscale.

=== Database Backups
I don't remember a time when the cljdoc DigitalOcean droplet has failed us.
It just keeps chugging along.

But hardware does fail and instances do go poof.
This might be more of a normal occurence at Exoscale, we don't know yet.

To compensate we should do what we should have been doing all along over at Digital Ocean.
We should be automatically periodically backing up our SQLite database.

In theory, the SQLite database can be wholly reconstituted by rebuilding docs.
But this represents a lot of compute time over at CircleCI so we'd rather save the hard work CircleCI has done for cljoc.

Our db backup is about 1gb and we want to be respectful of Exoscale resources, we don't need to keep all backups.
A daily backup should be sufficient with backup retention strategy of:
* 7 daily
* 4 weekly
* 12 monthly
* 2 annually

Our Lucene full-text database is quickly reconstituted from clojars at startup time, so no need to save a backup of it.

=== Packer or Cloud Init?
We currently use packer to build our host image.

Exoscale offers a nice selection pre-built image templates.
I've explored using a Debian pre-built template, then adding what docker, nomad and consul, etc via cloud init.

I've successfuly done this, but given the cloud init docs are on the less coherent side, it took me quite a while to figure out.
And while cloud init works, the updates are applied after the image boots.
So there will be some necessary waiting until cloud init completes.

My feeling is that cloud init might have its place for light config, but packer is the better choice for installing requisite packages.

Although Exoscale documents using Packer, its not listed as a Packer integrations
https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean - here's digitalocean
https://github.com/exoscale/packer-plugin-exoscale - aha! here it is.

After some experimentation, my feeling is that for initial software setup, packer is easier to verify and work with.
We can work an existing Exoscale template as a base (we'll use Debian 12 to start with) and build upon it.

Packer notes:
- For DigitalOcean we embedded the date in the DigitalOcean image identifier.
For Exoscale we won't do this.
Exoscale allows for multiple private templates with the same name and will automatically pick the most recent one.
This is perhaps a bit less human-friendly and concrete but avoids having to discover/store the current template which would add complexity when there is more than 1 ops person on the ops team.

=== Deployment
See `modules/deploy` for the details.

On deploy:
- ensure docker hub has cljdoc docker image for this release
- use ssh port forwarding to cljdoc host server
- sync config via consul API
  - traefik config `config/traefik-toml`
  - cljdoc secrets `config/cljdoc/secrets-edn`
- post our jobspec to nomad API
  - lb (gets is config from consul)
  - cljdoc (with docker tag of release) (gets secrets from consul)
- wait until new cljdoc deployment is healthy (via nomad)
- promote new deployment via nomad
  - canary becomes cljdoc
  - and old cljdoc retired

I think I might be able to mostly just reuse this.
The consul and nomad REST APIs, I think, are still supported and valid.

=== Traefik
We'll continue to use traefik as our internal load balancer to support blue/green deployments.
Traefik is currently at v3.1.2, we are bit behind.

Traefik is run from a docker image (known to nomad as `lb`).

What is traefik's role?:
- redirects cljdoc.xyz to cljdoc.org
- SSL certs via Let's Encrypt? (configured under `acme`)
- directs traffic to consul discovered cljdoc

Reminder: traefik logs exhausted all disk space over at DigitalOcean and caused nomad corruption; we probably want to implement traefik log rotation and deletion.
Maybe save 2 weeks of logs?

=== Nomad & Consul Initial Config
Cljdoc's DigitalOcean Packer config installed

- `/ect/nomad/server.hcl`
- `/etc/systemd/system/nomad.service`
- `/etc/systemd/system/consul.service`

I don't know if these were overriding existing default configs or providing a config where non existed.
There were changes some of these files, so I assume those changes will need to be included/replicated.
Will have to review against current versions of nomad and consul.

I'm noticing that config on the actual server has somehow drifted from what we have in terraform.
Actual config `etc/nomad/server/hcl`:

[source,hcl]
---
data_dir = "/etc/nomad.d"

server {
  enabled          = true
  bootstrap_expect = 1
}

client {
  enabled = true
}

plugin "docker" {
  config {
    volumes {
      enabled      = true
      selinuxlabel = "z"
    }
  }
}
----

Some changes I'll make:
- create `consul` user for consul service
- nomad docs say it should be run as root https://developer.hashicorp.com/nomad/docs/operations/nomad-agent
so continue to do so
- use `/etc/nomad.d` for config dir, and `/etc/consul.d` as home and config dir
- use `/var/lib/nomad` and `/var/lib/consul` for data dirs

Some notes:
- nomad complains about Serf comms, but I think this is ignorable for a single-node installation?

=== Zip Release Artifacts
The release workflow creates a zip file from which it then creates a docker image which it then uploads to docker hub.

Each release uploads the zip file to s3.
I'm not entirely sure of the value of this.
I does keep a record of what actually built the cljdoc docker image with.
I suppose we could carry on with this.

=== Pinning Packages
Historically hashicorp seems to have had no qualms about introducing breaking changes.

So rather than installing the latest, we probably want to install and pin `nomad` and `consul` versions.

For docker, maybe just pin to current major.

It might be interesting at some future date to look into NixOS.

=== Deploying from CircleCI
I see that we deploy to `NOMAD_IP`, I don't think this would resolve to something different than cljdoc.org.
This implies we have a static IP setup at DigitalOcean.

We can setup a static IP on Exoscale via Elastic IPs.
https://community.exoscale.com/documentation/compute/eip/

If we define our static IP via terraform, we'll have to remember that if we `destroy` this aspect of our setup, we'll also be destroying static IP.
I'm not sure how this is expressed in the current terraform config; if it is.

=== Critical Updates
Sometimes vulnerabilities are discovered.
How to address?

=== Logs
When currently send error level log events to Sentry.io.
We make no effort to save any other logs.
Which could be OK for cljdoc.

I've sometimes taken a peek a cljdoc logs via nomad.
But otherwise, I've been uninterested.

Other than addressing traefik's log rotation, I'll likely not make any changes, at least initially, when moving to Exoscale.

=== Firewall
Exoscale has firewall support via security groups.

I see that our DigitalOcean droplet also setup firewalld.
I'll look into both of these.

== Thoughts & Notes from Experiments

=== Cloud Init is Tough to Test
I started off testing by launching Compute instances at Exoscale, but that was becoming painful.

I landed on testing locally with lxd.

Installation: https://support.system76.com/articles/containers/
(missing cmd: newgrp lxd).

Initial setup (rerun after delete):
[source,shell]
----
lxc launch images:debian/12 debian12
----

Other useful commands
[source,shell]
----
lxc stop debian12
lxc delete debian12
lxc restart debian12
----

The base debian is missing cloud init so we have to install it first
[source,shell]
----
lxc exec debian12 -- apt update
lxc exec debian12 -- apt install cloud-init
----

And then feed our cloud init config, then restart for it to take effect:
[source,shell]
----
lxc config set debian12 user.user-data - < cloud-config.yaml
lxc restart debian12
----

Useful cmds to snoop around:
[source,shell]
----
lxc exec debian12 -- cat /var/log/cloud-init.log
lxc exec debian12 -- cat /var/log/cloud-init-output.log
lxc exec debian12 -- /bin/bash
----

Useful cloud-init cmds:

* `cloud-init status` - Reports `status: done` when complete
* `cloud-init status --wait` - Waits for cloud-init to complete all tasks then reports status
